<!DOCTYPE html>
<html lang="en">

  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-LX9GVX80T8"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
    
      gtag('config', 'G-LX9GVX80T8');
    </script>
    <title> Sai Teja Gilukara</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <!--<title>Agency - Start Bootstrap Theme</title>-->

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="vendor/fontawesome-free/css/all.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- Custom styles for this template -->
    <link href="css/agency.min.css" rel="stylesheet">
    <link href="css/profile-pic-container.css" rel="stylesheet">

  </head>

  <body id="page-top">

    <!-- Navigation -->
    <nav class="navbar navbar-expand-lg navbar-dark fixed-top" id="mainNav">
      <div class="container">
        <a class="navbar-brand js-scroll-trigger" href="#page-top">Sai Teja Gilukara</a>
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
          Menu
          <i class="fas fa-bars"></i>
        </button>
        <div class="collapse navbar-collapse" id="navbarResponsive">
          <ul class="navbar-nav text-uppercase ml-auto">
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#portfolio">Projects</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#TechnicalSkills">Skills</a>
            </li>
            <li class="nav-item">
              <a class="nav-link js-scroll-trigger" href="#team">Contact Me</a>
            </li>
          </ul>
        </div>
      </div>
    </nav>

    <!-- Header -->
    <header class="masthead">
      <div class="container">
        <div class="intro-text">           
          <div class="intro-heading text-uppercase"style="background-color: rgba(26, 26, 26, 0.7); color:rgba(253, 201, 43, 1); border-radius:5px;">Sai Teja Gilukara</div>
          <div class="intro-lead-in">Building AI and Robotics Systems to Solve Real Life Challenges</div>
          <a class="btn btn-primary btn-xl text-uppercase js-scroll-trigger" href="https://drive.google.com/file/d/1BHSwRa_hNdR2I7TN-illwwK-8fbIzNwO/view?usp=sharing">Resume</a>
        </div>
      </div>
    </header>

    <!-- Portfolio Grid -->
    <section class="bg-light" id="portfolio">
      <div class="container">
        <div class="row">
          <div class="col-lg-12 text-center">
            <h2 class="section-heading text-uppercase">Projects</h2>
            <!-- <h3 class="section-subheading text-muted">September 2019 ~ Now</h3> -->
          </div>
        </div>
        <div class="row">

         <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#musclesynergy">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Quiz\quiz.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Mission Quizify</h4>
              <p class="text-muted">Large Language Model | Vertex AI <br> Generate personalized quiz</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#healerbaxter">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Blind\blind.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Visually Impaired Navigation Assistance
              </h4>
              <p class="text-muted">Deep Learning | CV | Transformers<br>Gives audio instructions in real time</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#terminator">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\Pose\pose_estimation.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>Body Pose Estimation on CPU</h4>
              <p class="text-muted">Deep Learning | CV | Transformers<br>Provides pose estimation with 18 keypoints</p>
            </div>
        </div>

            <!-- <div class="col-md-4 col-sm-6 portfolio-item">
                <a class="portfolio-link" data-toggle="modal" href="#kuka">
                  <div class="portfolio-hover">
                    <div class="portfolio-hover-content">
                      <i class="fas fa-plus fa-3x"></i>
                    </div>
                  </div>
                  <img class="img-fluid" src="img/portfolio/manipulation_proj/manipulation.gif" alt="" width="400" height="300">
                </a>
                <div class="portfolio-caption">
                  <h4>Agile Robotics for Industrial Automation Competition </h4>
                  <p class="text-muted">C++ | ROS | Python | OpenCV<br> Build intelligent robot applications to solve unseen scenarios with failure  </p>
                </div>
              </div> -->

              <div class="col-md-4 col-sm-6 portfolio-item">
                <a class="portfolio-link" data-toggle="modal" href="#kuka">
                  <div class="portfolio-hover">
                    <div class="portfolio-hover-content">
                      <i class="fas fa-plus fa-3x"></i>
                    </div>
                  </div>
                  <img class="img-fluid" src="img\portfolio\Autodock\Auto Dock.gif" alt="" width="400" height="300">
                </a>
                <div class="portfolio-caption">
                  <h4>Self Docking </h4>
                  <p class="text-muted">C++ | ROS | Python | OpenCV<br> Build intelligent robot applications to solve unseen scenarios with failure  </p>
                </div>
              </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#pushinggrasping">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img\portfolio\GAN\image.png" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>CBAGAN-RRT: Convolutional Block Attention GAN for Sampling-Based Path Planning</h4>
              <p class="text-muted">CNN | Path Planning |RRT* <br>Deep Learning approach for Sampling-Based algorithm</p>
            </div>
          </div>

          <div class="col-md-4 col-sm-6 portfolio-item">
            <a class="portfolio-link" data-toggle="modal" href="#panorama">
              <div class="portfolio-hover">
                <div class="portfolio-hover-content">
                  <i class="fas fa-plus fa-3x"></i>
                </div>
              </div>
              <img class="img-fluid" src="img/portfolio/Image_mosaic/panorama.gif" alt="" width="400" height="300">
            </a>
            <div class="portfolio-caption">
              <h4>ImageMosaic</h4>
              <p class="text-muted">Computer Vision<br>Create an image panorama by stitching a set
                of images together</p>
            </div>
          </div>
          
        </div>
      </div>
    </section>

    <!-- Technical Skills -->
    <section  id="TechnicalSkills">
        <div class="container">
          <div class="row">
            <div class="col-lg-12 text-center">
              <h2 class="section-heading text-uppercase">Skills</h2>
              <p></p>
            </div>
          </div>
          <div class="row text-center">
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/microcontroller.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Robotics</h4>
                <p class="text-muted"> ROS,Linux,Git,Gazebo <br> Computer Vision, Machine Learning<br> Motion Planning, Feedback Control</p>
            </div>
            <div class="col-md-4">
                <span class="fa-stack fa-4x">
                  <img class="mx-auto rounded-circle" src="img/skills/coding.jpeg" alt="" width="120" height="120">  
                </span>
                <h4 class="service-heading">Programming</h4>
                <p class="text-muted">C++, C，Python, MATLAB, Arduino, Doxygen, LATEX, HTML, SQL</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <!-- <i class="fas fa-circle fa-stack-2x text-primary"></i> -->
                <img class="mx-auto rounded-circle" src="img/skills/ai.webp" alt="" width="120" height="120">
              </span>
              <h4 class="service-heading">Deep Learning</h4>
              <p class="text-muted">CNN, GANs, Transfer Learning, Attention Mechanisms, transformers, Pytorch, CUDA</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/manufacturing.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Manufacturing</h4>
              <p class="text-muted"> Laser Cutter, 3D print, soldering </p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/cad.png" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Mechanical Engineering</h4>
              <p class="text-muted">AutoCAD, FreeCAD, SolidWorks</p>
            </div>
            <div class="col-md-4">
              <span class="fa-stack fa-4x">
                <img class="mx-auto rounded-circle" src="img/skills/machine_learning.jpeg" alt="" width="120" height="120">  
              </span>
              <h4 class="service-heading">Research Interest</h4>
              <p class="text-muted">Autonomous Robotics, Multi Robot Systems, Path Planning, Perception and Controls</p>
            </div>
          </div>
        </div>
      </section>
    
    <!-- contact -->
    <section class="bg-light" id="team">
      <div class="container">
        <div class="row">
            <div class="col-lg-12 max-auto mb-5">
                <!-- Use a container div for the image with a specific class -->
                <div class="profile-pic-container mx-auto d-block">
                    <!-- Add only the class for the image, remove inline styles -->
                    <img class="profile-pic" src="img/profilepic.jpg" alt="Avatar">
                </div>
            </div>
            <div class="col-lg-12 text-center">
                <h2 class="section-heading text-uppercase">Contact me</h2>
            </div>
        </div>
    </div>
        <div class="row">
          <div class="col-lg-12">
         

            <div class="team-member">
              
              <h4>Sai Teja Gilukara</h4>
              <p class="text-muted">Master of Engineering in Robotics @ University of Maryland, College Park</p>
              <ul class="list-inline social-buttons">
                <li class="list-inline-item">
                  <a href="https://github.com/saiteja12-g">
                    <i class="fab fa-github"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="mailto:saitejag@umd.edu">
                    <i class="fa fa-envelope"></i>
                  </a>
                </li>
                <li class="list-inline-item">
                  <a href="https://www.linkedin.com/in/sai-teja-gilukara-081a791b0/">
                    <i class="fab fa-linkedin-in"></i>
                  </a>
                </li>
              </ul>
            </div>
          </div>
          </div>
        </div>
        <div class="row">
          <div class="col-lg-8 mx-auto text-center">
           
          </div>
        </div>
      </div>
    </section>


    <!-- Portfolio Modals -->
    <!-- Mission Quizify -->
    <div class="portfolio-modal modal fade" id="musclesynergy" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Mission Quizify</h3>
                    <a class="boxed">LLM</a>&ensp;<a class="boxed">Vertex AI</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Streamlit</a>&ensp;
                    <p></p><br>
                    <h4>Brief Description</h4>
                    <p></p>     
                    <p>Mission Quizify is an advanced AI-driven platform designed to enhance educational experiences through personalized quizzes. Developed at Radical AI, New York, NY, this project leverages cutting-edge AI technologies to generate and deliver customized educational content that caters to the unique learning needs of each user.
                    </p>
                    <h4>Technologies Used</h4>  
                    <p></p>
                    <!-- <img class="img-fluid" src="img/portfolio/visually/demo.mp4" alt="" width="400" height="250">   -->
                    <!-- <p>Figure 1</p>                -->
                    <ul>
                      <li>Python: Primary programming language for backend development.</li>
                      <li>Streamlit: Built a user-friendly web interface allowing seamless user interactions.</li>
                      <li>Google Cloud Vertex AI: Powers advanced question generation </li>
                      <li>Chroma DB: Utilized for robust data handling and ensuring reliable quiz delivery</li>
                    </ul>
                    <h4>Features</h4>
                    <ul>
                      <li>Personalized Quiz Generation: Utilizes Vertex AI to create quizzes</li>
                      <li> User Interface: Built with Streamlit, interface provides a smooth user interaction model.</li>
                    </ul>
                    <p></p>
                    <h6>Demo</h6>
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/rYIes6g6AWM?si=MvHefWeu2HT9B_lP" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    <p></p>
                    <p><a class="boxed" href="https://github.com/saiteja12-g/mission-quizify"><font color="red"><b>Github Page</b> </font></a></p>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

    <!-- Visually Impaired -->
    <div class="portfolio-modal modal fade" id="healerbaxter" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
          <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
              <div class="lr">
                <div class="rl"></div>
              </div>
            </div>
            <div class="container">
              <div class="row">
                <div class="col-lg-8 mx-auto">
                  <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Visually Impaired Navigation Assistance                    </h3>
                    <a class="boxed">Deep Learning</a>&ensp;<a class="boxed">Tensorflow</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Computer Vision</a>&ensp;<a class="boxed">LLM</a>
                    <p></p>
                    <h4>Brief Description</h4>       
                  	<p>This research introduces a transformative approach to
                      empower visually impaired individuals in navigating their
                      surroundings independently. We present a real-time scene
                      description system employing the innovative ExpansionNet
                      v2 model and a user-friendly app. This groundbreaking
                      technology achieves an impressive 85% accuracy in providing dynamic audio descriptions of scenes, significantly enhancing the mobility of visually impaired individuals both
                      indoors and outdoors.</p>
                  	<p></p>
                    <h5>3. Proposed CNN Model</h5>       
                  	<p>ExpansionNet v2 utilizes a modified version of the Vision Transformer (ViT) architecture as its backbone. ViT’s
                      inherent ability to model long-range dependencies and relationships within data makes it well-suited for handling
                      the expanded blocks and heterogeneous sequences generated by BSE. The training strategy employs a two-stage approach:
                      <li> Stage 1: Cross-entropy pre-training: The model is trained
                      on both the original image and its expanded blocks, along
                      with captions generated from them. This stage utilizes
                      cross-entropy loss to minimize the difference between the
                      model’s generated captions and the actual captions, fostering basic image understanding and caption generation
                      skills.</li>

                      <li>• Stage 2: Reinforcement learning fine-tuning: In the final
                      stage, BLEU score guides the model’s learning through
                      reinforcement learning. This incentivizes the model to
                      generate captions that are not only factually accurate but
                      also fluent, grammatically correct, and engaging. This
                      stage polishes the model’s skills, ensuring its captions effectively capture the essence of the image</li> </p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/Blind/image.png" alt="">
                    <p></p>
                  	<h5>Demo</h5> 
                    <iframe width="560" height="315" src="https://www.youtube.com/embed/6QHPmOYY21s?si=-m6LiV1RsPkqcQ_a" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://drive.google.com/file/d/1KMaTStvABzm0J1G96Zebo36merviPlv0/view?usp=sharing"><font color="red"><b>Report</b> </font></a></p>
                    </ul>
                    <ul class="list-inline">
                      <p><a class="boxed" href="https://github.com/saiteja12-g/Helping-Blind-People"><font color="red"><b>Github Page</b> </font></a></p>
                      </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                      <i class="fas fa-times"></i>
                      Close Project</button>
                      
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
  
    <!-- Body Pose Estimation -->
    <div class="portfolio-modal modal fade" id="terminator" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Body Pose Estimation on CPU</h3>
                  <a class="boxed">Deep Learning</a>&ensp;<a class="boxed">CV</a>&ensp;<a class="boxed">Python</a>&ensp;<a class="boxed">Transformers</a>
                  <p></p>         
                  <h4>Brief Description</h4>       
                  <p>This project focuses on developing a deep learningbased approach for human pose estimation using the popular
                    COCO dataset. Leveraging the advanced capabilities of
                    convolutional neural networks (CNNs), the project implements
                    a pose estimation model that accurately identifies and
                    tracks human body keypoints in images. The core of the
                    project is built around a tailored architecture based on the
                    PoseEstimationWithMobileNet model, which is known for its
                    efficiency and accuracy in processing images for keypoint
                    detection.
                    The project’s outcome demonstrates the model’s proficiency in
                    detecting human poses with high accuracy. The results are quantified using standard metrics and visually represented to show
                    the model’s effectiveness. This report encapsulates the journey
                    from conceptualization to implementation, offering insights into
                    the challenges faced and the innovative solutions adopted. The
                    project sets a foundation for future enhancements in real-time
                    applications and more complex pose estimation challenges</p>
                  <h4>Architecture</h4> 
                  <img class="img-fluid" src="img\portfolio\Pose\image.png" alt="" width="1600" height="1200">
                  <p></p>
                  <h4>Demo</h4>
                  <img class="img-fluid" src="img\portfolio\Pose\pose_estimation.gif" alt="" width="400" height="300">
                  <p></p>

                  <ul class="list-inline">
                  <p><a class="boxed" href="https://github.com/saiteja12-g/Light-weight-Pose-Estimation-on-CPU.git"><font color="red"><b>Github Page</font></a></p> </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>


    <!-- Self Docking Robot -->
    <div class="portfolio-modal modal fade" id="kuka" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">Self Docking Robot</h3>
                  <a class="boxed">Manipulation</a>&ensp;<a class="boxed">Motion Planning</a>&ensp;<a class="boxed">VREP</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>This project involves the development of an autonomous robot system capable of independently locating its charging station, planning a collision-free path, and accurately aligning itself with the docking station for charging. The entire system is built using ROS2 (Robot Operating System Version 2), which provides a robust framework for robot software development, enhancing real-time performance and supporting more complex and distributed systems.
                  </p>                           
                  <h4>Pipeline</h4>
                  <img class="img-fluid d-block mx-auto" src="img\portfolio\Autodock\image.png" alt="">
                  <p>The goal of this project is to drive the KUKA youBot to pick up a block at the start location, carry it to the desired location, and put it down in the simulation software V-REP. The project covers the following topics: <br>1. Plan a trajectory for the end-effector of the youBot mobile manipulator. <br>2. Generate the kinematics model of the youBot, consisting of the mobile base with 4 mecanum wheels and the robot arm with 5 joints<br>3. Apply feedback control to drive the robot to implement the desired task<br>4. Conduct the simulations in V-REP</p>                           
                  
                  <h5>First Task Demo - with RGB</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/QAEuheAQ5iE?si=8SNKLJKZZ0L-Lrfk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  <h5>Second Task Demo - with LIDAR</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/6vVA5_HM5PY?si=3rFH3HcGUA_6bJr0" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>                  <p><a class="boxed" href="https://github.com/mushenghe/Mobile-Manipulation-"><font color="red"><b>Github Page</font></a></p>
                  <h5>Third Task Demo - Hardware Implementation</h5>
                  <iframe width="560" height="315" src="https://www.youtube.com/embed/_IWPu9QREV0?si=G5zGkE38FI7ZDdId" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
                  <p><a class="boxed" href="https://github.com/abhinavsagar/docking_673"><font color="red"><b>Github Page</font></a></p> </ul>
                  
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Visual Pushing and Grasping -->
    <div class="portfolio-modal modal fade" id="pushinggrasping" tabindex="-1" role="dialog" aria-hidden="true">
        <div class="modal-dialog">
            <div class="modal-content">
            <div class="close-modal" data-dismiss="modal">
                <div class="lr">
                <div class="rl"></div>
                </div>
            </div>
            <div class="container">
                <div class="row">
                <div class="col-lg-8 mx-auto">
                    <div class="modal-body">
                    <!-- Project Details Go Here -->
                    <h3 class="text-uppercase">Visual Pushing and Grasping</h3>
                    <a class="boxed">Deep Reinforcement Learning</a>&ensp;<a class="boxed">Docker</a>&ensp;<a class="boxed">Pytorch</a>
                    <p></p>
                    <h4>Brief Description</h4> 
                    <p>Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common
                      problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is
                      too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm
                      (CBAGAN-RRT) using a Convolutional Block Attention
                      Generative Adversarial Network with a combination of
                      spatial and channel attention and a novel loss function
                      to design the heuristics, find a better optimal path, and
                      improve the convergence of the algorithm both concerning time and speed. The probability distribution of the
                      paths generated from our GAN model is used to guide the
                      sampling process for the RRT algorithm. We train and
                      test our network on the dataset generated by (Zhang et al.,
                      2021) and demonstrate that our algorithm outperforms
                      the previous state-of-the-art algorithms using both the
                      image quality generation metrics like IOU Score, Dice
                      Score, FID score, and path planning metrics like time
                      cost and the number of nodes.</h5> 
                    <p></p>
                    <p></p>
                    <!-- <img class="img-fluid d-block mx-auto" src="img/portfolio/pushing_grasping/method.jpg" alt=""> -->
                    <p></p>
                    <h5>Dataset and Augmentation</h5>
                    <p></p>
                    <p>We used the dataset generated by (Zhang et al., 2021)
                      to validate our results. The dataset was generated by
                      randomly placing different obstacles on the map and
                      randomly sampling the start and goal nodes which are
                      denoted by red and blue dots on the map respectively.
                      The RRT algorithm was run to generate the feasible path
                      which is shown in green color or the ground truth. The
                      dimensions of all the images are (3x256x256) where the
                      height and the width of the images are 256 and the number of channels is 3. We use 8000 images for training and
                      2000 images for testing respectively using the dataset by
                      (Zhang et al., 2021).</p>
                    <p></p>
                    <p>The parameters used in the data augmentation like height
                      shift of the map, width shift of the map, shift step of the
                      map, rotation probability of the map, and the number of
                      maps generated of the map are shown below</p>
                    <p></p>
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\data_aug.png" alt="">
                    <p></p>
                    <h5>Architecture</h5>
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\arch.png" alt="">
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\RRT algo.png" alt=""> 
                    <h5>Loss Function</h5>
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\loss.png" alt="">
                    <!-- <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\RRT algo.png" alt="">  -->
                    <h5>Results</h5>
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\AOI.png" alt="">
                    <img class="img-fluid d-block mx-auto" src="img\portfolio\GAN\final.png" alt="">                    
                    <ul class="list-inline">
                    <p><a class="boxed" href="https://github.com/saiteja12-g/CBAGAN-RRT"><font color="red"><b>Github Page</font></a></p> 
                    </ul>
                    <button class="btn btn-primary" data-dismiss="modal" type="button">
                        <i class="fas fa-times"></i>
                        Close Project</button>
                    </div>
                </div>
                </div>
            </div>
            </div>
        </div>
        </div>
      

    <!-- Panorama -->
    <div class="portfolio-modal modal fade" id="panorama" tabindex="-1" role="dialog" aria-hidden="true">
      <div class="modal-dialog">
        <div class="modal-content">
          <div class="close-modal" data-dismiss="modal">
            <div class="lr">
              <div class="rl"></div>
            </div>
          </div>
          <div class="container">
            <div class="row">
              <div class="col-lg-8 mx-auto">
                <div class="modal-body">
                  <!-- Project Details Go Here -->
                  <h3 class="text-uppercase">ImageMosaic</h3>
                  <a class="boxed">Computer Vision</a>&ensp;<a class="boxed">MATLAB</a>&ensp;<a class="boxed">Python</a>
                  <p></p>
                  <h4>Brief Description</h4>
                  <p>The goal of this project is to Create an image panorama by stitching a set of images together</p>                           
                  <h5>Image Registration</h5>
                  <p>I used SURF to do the feature point extraction and matching, then used random sample consensus(RANSAC) for transform matrix estimation</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/SURFMatches.jpg" alt="">
                  <h5>Image Warping</h5>
                  <p>Use the derived transform matrix nad project that warped image on a plain surface</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/img1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/image_mosaic/img2.jpg" alt="">
                  <h5>Image Blending</h5>
                  <p>Using Center-Weighting algorithm (compute the the distance from each pixel to 4 boundaries of the image and take the the smallest ratio                   
                    between two distances and the dimension of image as the corresponding pixel
                    value on mask matrix). The mask we derived is shown in the following image:</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/mask.jpg" alt="">
                  <p>For each image, I derive a mask and then warp the mask just as warp the image</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/before.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/after.jpg" alt="">
                  <h5>Cropping</h5>
                  <p>After doing image stitching and image blending, I get the panorama look as following</p>
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/1.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/2.jpg" alt="">
                  <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/3.jpg" alt="">
                  <p>Use pythong to find the largest rectangle that don’t include the black region in the
                    panorama image, I get the final panorama look as following</p>
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/1_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/2_a.jpg" alt="">
                    <img class="img-fluid d-block mx-auto" src="img/portfolio/Image_mosaic/3_a.jpg" alt="">
                  <ul class="list-inline">
                  <!-- <p><a class="boxed" href="https://drive.google.com/file/d/1p8h66kcTu3J3DAQ-OETgzaHgj101Kw8o/view?usp=sharing"><font color="red"><b>Report</font></a></p> -->
                  </ul>
                  <button class="btn btn-primary" data-dismiss="modal" type="button">
                    <i class="fas fa-times"></i>
                    Close Project</button>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
   

    <!-- Bootstrap core JavaScript -->
    <script src="vendor/jquery/jquery.min.js"></script>
    <script src="vendor/bootstrap/js/bootstrap.bundle.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="vendor/jquery-easing/jquery.easing.min.js"></script>

    <!-- Contact form JavaScript -->
    <script src="js/jqBootstrapValidation.js"></script>
    <script src="js/contact_me.js"></script>

    <!-- Custom scripts for this template -->
    <script src="js/agency.min.js"></script>

  </body>

</html>
